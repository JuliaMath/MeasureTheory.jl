var documenterSearchIndex = {"docs":
[{"location":"old_readme/#MeasureTheory","page":"MeasureTheory","title":"MeasureTheory","text":"","category":"section"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"(Image: Stable) (Image: Dev) (Image: Build Status) (Image: Coverage)","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"Check out our JuliaCon submission","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"MeasureTheory.jl is a package for building and reasoning about measures.","category":"page"},{"location":"old_readme/#Why?","page":"MeasureTheory","title":"Why?","text":"","category":"section"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"A distribution (as in Distributions.jl) is also called a probability measure, and carries with it the constraint of adding (or integrating) to one. Statistical work usually requires this \"at the end of the day\", but enforcing it at each step of a computation can have considerable overhead.","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"As a generalization of the concept of volume, measures also have applications outside of probability theory.","category":"page"},{"location":"old_readme/#Goals","page":"MeasureTheory","title":"Goals","text":"","category":"section"},{"location":"old_readme/#Distributions.jl-Compatibility","page":"MeasureTheory","title":"Distributions.jl Compatibility","text":"","category":"section"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"Distirbutions.jl is wildly popular, and is large enough that replacing it all at once would be a major undertaking. ","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"Instead, we should aim to make any Distribution easily usable as a Measure. We'll most likely implement this using an IsMeasure trait. ","category":"page"},{"location":"old_readme/#Absolute-Continuity","page":"MeasureTheory","title":"Absolute Continuity","text":"","category":"section"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"For two measures μ, ν on a set X, we say μ is absolutely continuous with respect to ν if ν(A)=0 implies μ(A)=0 for every measurable subset A of X.","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"The following are equivalent:","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"μ ≪ ν\nμ is absolutely continuous wrt ν\nThere exists a function f such that μ = ∫f dν","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"So we'll need a ≪ operator. Note that ≪ is not antisymmetric; it's common for both μ ≪ ν and  ν ≪ μ to hold. ","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"If μ ≪ ν and  ν ≪ μ, we say μ and ν are equivalent and write μ ≃ ν. (This is often written as μ ~ ν, but we reserve ~ for random variables following a distribution, as is common in the literature and probabilistic programming languages.)","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"If we collapse the equivalence classes (under ≃), ≪ becomes a partial order.","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"We need ≃ and ≪ to be fast. If the support of a measure can be determined statically from its type, we can define ≃ and ≪ as generated functions. ","category":"page"},{"location":"old_readme/#Radon-Nikodym-Derivatives","page":"MeasureTheory","title":"Radon-Nikodym Derivatives","text":"","category":"section"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"One of the equivalent conditions above was \"There exists a function f such that μ = ∫f dν\". In this case, f is called a Radon-Nikodym derivative, or (less formally) a density. In this case we often write f = dμ/dν.","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"For any measures μ and ν with μ≪ν, we should be able to represent this.","category":"page"},{"location":"old_readme/#Integration","page":"MeasureTheory","title":"Integration","text":"","category":"section"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"More generally, we'll need to be able to represent change of measure as above, ∫f dν. We'll need an Integral type","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"struct Integral{F,M}\n    f::F\n    μ::M\nend","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"Then we'll have a function ∫. For cases where μ = ∫f dν,  ∫(f, ν) will just return μ (we can do this based on the types). For unknown cases (which will be most of them), we'll return ∫(f, ν) = Integral(f, ν).","category":"page"},{"location":"old_readme/#Measure-Combinators","page":"MeasureTheory","title":"Measure Combinators","text":"","category":"section"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"It should be very easy to build new measures from existing ones. This can be done using, for example, ","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"restriction\nproduct measure\nsuperposition\npushforward","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"There's also function spaces, but this gets much trickier. We'll need to determine a good way to reason about this.","category":"page"},{"location":"old_readme/#More???","page":"MeasureTheory","title":"More???","text":"","category":"section"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"This is very much a work in progress. If there are things you think we should have as goals, please add an issue with the goals label.","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"","category":"page"},{"location":"old_readme/#Old-Stuff","page":"MeasureTheory","title":"Old Stuff","text":"","category":"section"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"WARNING: The current README is very developer-oriented. Casual use will be much simpler","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"For an example, let's walk through the construction of src/probability/Normal.","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"First, we have","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"@measure Normal","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"this is just a little helper function, and is equivalent to","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"TODO: Clean up","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"quote\n    #= /home/chad/git/Measures.jl/src/Measures.jl:55 =#\n    struct Normal{var\"#10#P\", var\"#11#X\"} <: Measures.AbstractMeasure{var\"#11#X\"}\n        #= /home/chad/git/Measures.jl/src/Measures.jl:56 =#\n        par::var\"#10#P\"\n    end\n    #= /home/chad/git/Measures.jl/src/Measures.jl:59 =#\n    function Normal(var\"#13#nt\"::Measures.NamedTuple)\n        #= /home/chad/git/Measures.jl/src/Measures.jl:59 =#\n        #= /home/chad/git/Measures.jl/src/Measures.jl:60 =#\n        var\"#12#P\" = Measures.typeof(var\"#13#nt\")\n        #= /home/chad/git/Measures.jl/src/Measures.jl:61 =#\n        return Normal{var\"#12#P\", Measures.eltype(Normal{var\"#12#P\"})}\n    end\n    #= /home/chad/git/Measures.jl/src/Measures.jl:64 =#\n    Normal(; var\"#14#kwargs\"...) = begin\n            #= /home/chad/git/Measures.jl/src/Measures.jl:64 =#\n            Normal((; var\"#14#kwargs\"...))\n        end\n    #= /home/chad/git/Measures.jl/src/Measures.jl:66 =#\n    (var\"#8#basemeasure\"(var\"#15#μ\"::Normal{var\"#16#P\", var\"#17#X\"}) where {var\"#16#P\", var\"#17#X\"}) = begin\n            #= /home/chad/git/Measures.jl/src/Measures.jl:66 =#\n            Lebesgue{var\"#17#X\"}\n        end\n    #= /home/chad/git/Measures.jl/src/Measures.jl:68 =#\n    (var\"#9#≪\"(::Normal{var\"#19#P\", var\"#20#X\"}, ::Lebesgue{var\"#20#X\"}) where {var\"#19#P\", var\"#20#X\"}) = begin\n            #= /home/chad/git/Measures.jl/src/Measures.jl:68 =#\n            true\n        end\nend","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"Next we have ","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"Normal(μ::Real, σ::Real) = Normal(μ=μ, σ=σ)","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"This defines a default. If we just give two numbers as arguments (but no names), we'll assume this parameterization.","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"Next need to define a eltype function. This takes a constructor (here Normal) and a parameter, and tells us the space for which this defines a measure. Let's define this in terms of the types of the parameters,","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"eltype(::Type{Normal}, ::Type{NamedTuple{(:μ, :σ), Tuple{A, B}}}) where {A,B} = promote_type(A,B)","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"That's still kind of boring, so let's build the density. For this, we need to implement the trait","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"@trait Density{M,X} where {X = domain{M}} begin\n    basemeasure :: [M] => Measure{X}\n    logdensity :: [M, X] => Real\nend","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"A density doesn't exist by itself, but is defined relative to some base measure. For a normal distribution this is just Lebesgue measure on the real numbers. That, together with the usual Gaussian log-density, gives us","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"@implement Density{Normal{X,P},X} where {X, P <: NamedTuple{(:μ, :σ)}} begin\n    basemeasure(d) = Lebesgue(X)\n    logdensity_def(d, x) = - (log(2) + log(π)) / 2 - log(d.par.σ)  - (x - d.par.μ)^2 / (2 * d.par.σ^2)\nend","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"Now we can compute the log-density:","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"julia> logdensity_def(Normal(0.0, 0.5), 1.0)\n-2.2257913526447273","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"And just to check that our default is working,","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"julia> logdensity_def(Normal(μ=0.0, σ=0.5), 1.0)\n-2.2257913526447273","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"What about other parameterizations? Sure, no problem. Here's a way to write this for mean μ (as before), but using the precision (inverse of the variance) instead of standard deviation:","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"eltype(::Type{Normal}, ::Type{NamedTuple{(:μ, :τ), Tuple{A, B}}}) where {A,B} = promote_type(A,B)\n\n@implement Density{Normal{X,P},X} where {X, P <: NamedTuple{(:μ, :τ)}} begin\n    basemeasure(d) = Lebesgue(X)\n    logdensity_def(d, x) = - (log(2) + log(π) - log(d.par.τ)  + d.par.τ * (x - d.par.μ)^2) / 2\nend","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"And another check:","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"julia> logdensity_def(Normal(μ=0.0, τ=4.0), 1.0)\n-2.2257913526447273","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"We can combine measures in a few ways, for now just scaling and superposition:","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"julia> 2.0*Lebesgue(Float64) + Normal(0.0,1.0)\nSuperpositionMeasure{Float64,2}((MeasureTheory.WeightedMeasure{Float64,Float64}(2.0, Lebesgue{Float64}()), Normal{NamedTuple{(:μ, :σ),Tuple{Float64,Float64}},Float64}((μ = 0.0, σ = 1.0))))","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"","category":"page"},{"location":"old_readme/","page":"MeasureTheory","title":"MeasureTheory","text":"For an easy way to find expressions for the common log-densities, see this gist","category":"page"},{"location":"affine/#Affine-Transformations","page":"Affine transformations","title":"Affine Transformations","text":"","category":"section"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"It's very common for measures to use parameters μ and σ, for example as in Normal(μ=3, σ=4) or StudentT(ν=1, μ=3, σ=4). In this context, μ and σ need not always refer to the mean and standard deviation (the StudentT measure specified above is equivalent to a Cauchy measure, so both mean and standard deviation are undefined).","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"In general, μ is a \"location parameter\", and σ is a \"scale parameter\". Together these parameters determine an affine transformation.","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"f(z) = σ z + μ","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"Starting with the above definition, we'll use z to represent an \"un-transformed\" variable, typically coming from a measure which has neither a location nor a scale parameter, for example Normal().","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"Affine transformations are often ambiguously referred as \"linear transformations\". In fact, an affine transformation is \"the composition of two functions: a translation and a linear map\" in the stricter algebraic sense: For a function f to be linear requires  f(ax + by) == a f(x) + b f(y) for scalars a and b. For an affine function f(z) = σ * z + μ, where the linear map is defined as σ and the translation defined as μ, linearity holds only if the translation component μ is equal to zero.","category":"page"},{"location":"affine/#Cholesky-based-parameterizations","page":"Affine transformations","title":"Cholesky-based parameterizations","text":"","category":"section"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"If the \"un-transformed\" z is univariate, things are relatively simple. But it's important our approach handle the multivariate case as well.","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"In the literature, it's common for a multivariate normal distribution to be parameterized by a mean μ and covariance matrix Σ. This is mathematically convenient, but leads to an O(n^3) Cholesky decomposition, which becomes a significant bottleneck to compute as n gets large.","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"While MeasureTheory.jl includes (or will include) a parameterization using Σ, we prefer to work in terms of its Cholesky decomposition σ.","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"To see the relationship between our σ parameterization and the likely more familiar  Σ parameterization,  let σ be a lower-triangular matrix satisfying","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"σ σᵗ = Σ","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"Then given a (multivariate) standard normal z, the covariance matrix of σ z + μ is","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"𝕍σ z + μ = Σ","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"The one-dimensional case where we have","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"𝕍σ z + μ = σ²","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"shows that the lower Cholesky factor of the covariance generalizes the concept of standard deviation, completing the link between σ and Σ.","category":"page"},{"location":"affine/#The-\"Cholesky-precision\"-parameterization","page":"Affine transformations","title":"The \"Cholesky precision\" parameterization","text":"","category":"section"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"The (μσ) parameterization is especially convenient for random sampling. Any measure z ~ Normal() determines an x ~ Normal(μ,σ) through the affine transformation","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"x = σ z + μ","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"The log-density transformation of a Normal with parameters μ, σ does not follow as directly. Starting with an x, we need to find z using","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"z = σ¹ (x - μ)","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"so the log-density is","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"logdensity_def(d::Normal{(:μ,:σ)}, x) = logdensity_def(d.σ \\ (x - d.μ)) - logdet(d.σ)","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"Here the - logdet(σ) is the \"log absolute Jacobian\", required to account for the stretching of the space.","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"The above requires solving a linear system, which adds some overhead. Even with the convenience of a lower triangular system, it's still not quite as efficient as multiplication.","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"In addition to the covariance Σ, it's also common to parameterize a multivariate normal by its precision matrix, defined as the inverse of the covariance matrix, Ω = Σ¹. Similar to our use of σ for the lower Cholesky factor of Σ, we'll use ω for the lower Cholesky factor of Ω.","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"This parameterization enables more efficient calculation of the log-density using only multiplication and addition,","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"logdensity_def(d::Normal{(:μ,:ω)}, x) = logdensity_def(d.ω * (x - d.μ)) + logdet(d.ω)","category":"page"},{"location":"affine/#AffineTransform","page":"Affine transformations","title":"AffineTransform","text":"","category":"section"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"Transforms like z  σ z + μ and z  ω  z + μ can be specified in MeasureTheory.jl using an AffineTransform. For example,","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"julia> f = AffineTransform((μ=3.,σ=2.))\nAffineTransform{(:μ, :σ), Tuple{Float64, Float64}}((μ = 3.0, σ = 2.0))\n\njulia> f(1.0)\n5.0","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"In the univariate case this is relatively simple to invert. But if σ is a matrix, matrix inversion becomes necessary. This is not always possible as lower triangular matrices are not closed under matrix inversion and as such are not guaranteed to exist. ","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"With multiple parameterizations of a given family of measures, we can work around these issues. The inverse transform of a (μσ) transform will be in terms of (μω), and vice-versa. So","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"julia> f⁻¹ = inverse(f)\nAffineTransform{(:μ, :ω), Tuple{Float64, Float64}}((μ = -1.5, ω = 2.0))\n\njulia> f(f⁻¹(4))\n4.0\n\njulia> f⁻¹(f(4))\n4.0","category":"page"},{"location":"affine/#Affine","page":"Affine transformations","title":"Affine","text":"","category":"section"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"Of particular interest (the whole point of all of this, really) is to have a natural way to work with affine transformations of measures. In accordance with the principle of \"common things should have shorter names\", we call this Affine.","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"The structure of Affine is relatively simple:","category":"page"},{"location":"affine/","page":"Affine transformations","title":"Affine transformations","text":"struct Affine{N,M,T} <: AbstractMeasure\n    f::AffineTransform{N,T}\n    parent::M\nend","category":"page"},{"location":"adding/#Adding-a-New-Measure","page":"Adding a new measure","title":"Adding a New Measure","text":"","category":"section"},{"location":"adding/#Parameterized-Measures","page":"Adding a new measure","title":"Parameterized Measures","text":"","category":"section"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"This is by far the most common kind of measure, and is especially useful as a way to describe families of probability distributions.","category":"page"},{"location":"adding/#Declaring-a-Parameterized-Measure","page":"Adding a new measure","title":"Declaring a Parameterized Measure","text":"","category":"section"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"To start, declare a @parameterized. For example, Normal is declared as","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"@parameterized Normal(μ,σ) ≪ (1/sqrt2π) * Lebesgue(ℝ)","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"[ℝ is typed as \\bbR <TAB>]","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"A ParameterizedMeasure can have multiple parameterizations, which as dispatched according to the names of the parameters. The (μ,σ) here specifies names to assign if none are given. So for example,","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"julia> Normal(-3.0, 2.1)\nNormal(μ = -3.0, σ = 2.1)","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"The right side, (1/sqrt2π) * Lebesgue(ℝ), gives the base measure. Lebesgue in this case is the technical name for the measure associating an interval of real numbers to its length. The (1/sqrt2π) comes from the normalization constant in the probability density function,","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"f_textNormal(xμσ) = frac1σ sqrt2 pi e^-frac12left(fracx-musigmaright)^2  ","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"Making this part of the base measure allows us to avoid including it in every computation.","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"The ≪ (typed as \\ll <TAB>) can be read \"is dominated by\". This just means that any set for which the base measure is zero must also have zero measure in what we're defining.","category":"page"},{"location":"adding/#Defining-a-Log-Density","page":"Adding a new measure","title":"Defining a Log Density","text":"","category":"section"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"Most computations involve log-densities rather than densities themselves, so these are our first priority. density(d,x) will default to exp(logdensity_def(d,x)), but you can add a separate method if it's more efficient.","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"The definition is simple:","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"logdensity_def(d::Normal{()} , x) = - x^2 / 2 ","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"There are a few things here worth noting.","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"First, we dispatch by the names of d (here there are none), and the type of x is not specified.","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"Also, there's nothing here about μ and σ. These location-scale parameters behave exactly the same across lots of distributions, so we have a macro to add them:","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"@μσ_methods Normal()","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"Let's look at another example, the Beta distribution. Here the base measure is Lebesgue(𝕀) (support is the unit interval). The log-density is","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"@inline function logdensity_def(d::Beta{(:α, :β)}, x)\n    return (d.α - 1) * log(x) + (d.β - 1) * log(1 - x) - logbeta(d.α, d.β)\nend","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"Note that when possible, we avoid extra control flow for checking that x is in the support. In applications, log-densities are often evaluated only on the support by design. Such checks should be implemented at a higher level whenever there is any doubt.","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"Finally, note that in both of these examples, the log-density has a relatively direct algebraic form. It's imnportant to have this whenever possible to allow for symbolic manipulation (using libraries like SymolicUtils.jl) and efficient automatic differentiation.","category":"page"},{"location":"adding/#Random-Sampling","page":"Adding a new measure","title":"Random Sampling","text":"","category":"section"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"For univariate distributions, you should define a Base.rand method that uses three arguments:","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"A Random.AbstractRNG to use for randomization,\nA type to be returned, and\nThe measure to sample from.","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"For our Normal example, this is","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"Base.rand(rng::Random.AbstractRNG, T::Type, d::Normal{()}) = randn(rng, T)","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"Again, for location-scale families, other methods are derived automatically. ","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"For multivariate distributions (or anything that requires heap allocation), you should instead define a Random.rand! method. This also takes three arguments, different from rand:","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"The Random.AbstractRNG,\nThe measure to sample from, and\nWhere to store the result.","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"For example, here's the implementation for ProductMeasure:","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"@propagate_inbounds function Random.rand!(rng::AbstractRNG, d::ProductMeasure, x::AbstractArray)\n    @boundscheck size(d.data) == size(x) || throw(BoundsError)\n\n    @inbounds for j in eachindex(x)\n        x[j] = rand(rng, eltype(x), d.data[j])\n    end\n    x\nend","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"Note that in this example, d.data[j] might itself require allocation. This implementation is likely to change in the future to make it easier to define nested structures without any need for ongoing allocation.","category":"page"},{"location":"adding/#Primitive-Measures","page":"Adding a new measure","title":"Primitive Measures","text":"","category":"section"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"Most measures are defined in terms of a logdensity with respect to some other measure, its basemeasure. But how is that measure defined? It can't be \"densities all the way down\"; at some point, the chain has to stop.","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"A primitive measure is just a measure that has itself as its own base measure. Note that this also means its log-density is always zero.","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"Here's the implementation of Lebesgue:","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"struct Lebesgue{X} <: AbstractMeasure end\n\nLebesgue(X) = Lebesgue{X}()\n\nbasemeasure(μ::Lebesgue) = μ\n\nisprimitive(::Lebesgue) = true\n\ngentype(::Lebesgue{ℝ}) = Float64\ngentype(::Lebesgue{ℝ₊}) = Float64\ngentype(::Lebesgue{𝕀}) = Float64\n\nlogdensity_def(::Lebesgue, x) = zero(float(x))","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"We haven't yet talked about gentype. When you call rand without specifying a type, there needs to be a default. That default is the gentype. This only needs to be defined for primitive measures, because others will fall back on ","category":"page"},{"location":"adding/","page":"Adding a new measure","title":"Adding a new measure","text":"gentype(μ::AbstractMeasure) = gentype(basemeasure(μ))","category":"page"},{"location":"api_index/#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"api_index/","page":"Index","title":"Index","text":"","category":"page"},{"location":"api_measurebase/#MeasureBase-API","page":"MeasureBase","title":"MeasureBase API","text":"","category":"section"},{"location":"api_measurebase/","page":"MeasureBase","title":"MeasureBase","text":"Modules = [MeasureBase]","category":"page"},{"location":"api_measurebase/#MeasureBase.Density","page":"MeasureBase","title":"MeasureBase.Density","text":"struct Density{M,B}\n    μ::M\n    base::B\nend\n\nFor measures μ and ν with μ≪ν, the density of μ with respect to ν (also called the Radon-Nikodym derivative dμ/dν) is a function f defined on the support of ν with the property that for any measurable a ⊂ supp(ν), μ(a) = ∫ₐ f dν.\n\nBecause this function is often difficult to express in closed form, there are many different ways of computing it. We therefore provide a formal representation to allow comptuational flexibilty.\n\n\n\n\n\n","category":"type"},{"location":"api_measurebase/#MeasureBase.DensityMeasure","page":"MeasureBase","title":"MeasureBase.DensityMeasure","text":"struct DensityMeasure{F,B} <: AbstractMeasure\n    density :: F\n    base    :: B\nend\n\nA DensityMeasure is a measure defined by a density with respect to some other \"base\" measure \n\n\n\n\n\n","category":"type"},{"location":"api_measurebase/#MeasureBase.Likelihood","page":"MeasureBase","title":"MeasureBase.Likelihood","text":"Likelihood(k::AbstractKleisli, x)\n\n\"Observe\" a value x, yielding a function from the parameters to ℝ.\n\nLikelihoods are most commonly used in conjunction with an existing prior measure to yield a new measure, the posterior. In Bayes's Law, we have\n\nP(θx)  P(θ) P(xθ)\n\nHere P(θ) is the prior. If we consider P(xθ) as a function on θ, then it is called a likelihood.\n\nSince measures are most commonly manipulated using density and logdensity, it's awkward to commit a (log-)likelihood to using one or the other. To evaluate a Likelihood, we therefore use density or logdensity, depending on the circumstances. In the latter case, it is of course acting as a log-density.\n\nFor example,\n\njulia> ℓ = Likelihood(Normal{(:μ,)}, 2.0)\nLikelihood(Normal{(:μ,), T} where T, 2.0)\n\njulia> density_def(ℓ, (μ=2.0,))\n1.0\n\njulia> logdensity_def(ℓ, (μ=2.0,))\n-0.0\n\nIf, as above, the measure includes the parameter information, we can optionally leave it out of the second argument in the call to density or logdensity. \n\njulia> density_def(ℓ, 2.0)\n1.0\n\njulia> logdensity_def(ℓ, 2.0)\n-0.0\n\nWith several parameters, things work as expected:\n\njulia> ℓ = Likelihood(Normal{(:μ,:σ)}, 2.0)\nLikelihood(Normal{(:μ, :σ), T} where T, 2.0)\n\njulia> logdensity_def(ℓ, (μ=2, σ=3))\n-1.0986122886681098\n\njulia> logdensity_def(ℓ, (2,3))\n-1.0986122886681098\n\njulia> logdensity_def(ℓ, [2, 3])\n-1.0986122886681098\n\n\n\nLikelihood(M<:ParameterizedMeasure, constraint::NamedTuple, x)\n\nIn some cases the measure might have several parameters, and we may want the (log-)likelihood with respect to some subset of them. In this case, we can use the three-argument form, where the second argument is a constraint. For example,\n\njulia> ℓ = Likelihood(Normal{(:μ,:σ)}, (σ=3.0,), 2.0)\nLikelihood(Normal{(:μ, :σ), T} where T, (σ = 3.0,), 2.0)\n\nSimilarly to the above, we have\n\njulia> density_def(ℓ, (μ=2.0,))\n0.3333333333333333\n\njulia> logdensity_def(ℓ, (μ=2.0,))\n-1.0986122886681098\n\njulia> density_def(ℓ, 2.0)\n0.3333333333333333\n\njulia> logdensity_def(ℓ, 2.0)\n-1.0986122886681098\n\n\n\nFinally, let's return to the expression for Bayes's Law, \n\nP(θx)  P(θ) P(xθ)\n\nThe product on the right side is computed pointwise. To work with this in MeasureBase, we have a \"pointwise product\" ⊙, which takes a measure and a likelihood, and returns a new measure, that is, the unnormalized posterior that has density P(θ) P(xθ) with respect to the base measure of the prior.\n\nFor example, say we have\n\nμ ~ Normal()\nx ~ Normal(μ,σ)\nσ = 1\n\nand we observe x=3. We can compute the posterior measure on μ as\n\njulia> post = Normal() ⊙ Likelihood(Normal{(:μ, :σ)}, (σ=1,), 3)\nNormal() ⊙ Likelihood(Normal{(:μ, :σ), T} where T, (σ = 1,), 3)\n\njulia> logdensity_def(post, 2)\n-2.5\n\n\n\n\n\n","category":"type"},{"location":"api_measurebase/#MeasureBase.PrimitiveMeasure","page":"MeasureBase","title":"MeasureBase.PrimitiveMeasure","text":"abstract type PrimitiveMeasure <: AbstractMeasure end\n\nIn the MeasureTheory ecosystem, a primitive measure is a measure for which the definition and construction do not depend on any other measure. Primitive measures satisfy the following laws:\n\nbasemeasure(μ::PrimitiveMeasure) = μ\n\nlogdensity_def(μ::PrimitiveMeasure, x) = 0.0\n\nlogdensity_def(μ::M, ν::M, x) where {M<:PrimitiveMeasure} = 0.0\n\n\n\n\n\n","category":"type"},{"location":"api_measurebase/#MeasureBase.SuperpositionMeasure","page":"MeasureBase","title":"MeasureBase.SuperpositionMeasure","text":"struct SuperpositionMeasure{X,NT} <: AbstractMeasure\n    components :: NT\nend\n\nSuperposition of measures is analogous to mixture distributions, but (because measures need not be normalized) requires no scaling. The superposition of two measures μ and ν can be more concisely written as μ + ν. Superposition measures satisfy\n\nbasemeasure(μ + ν) == basemeasure(μ) + basemeasure(ν)\n\n    beginalignedfracmathrmd(mu+nu)mathrmd(alpha+beta)  =fracfmathrmdalpha+gmathrmdbetamathrmdalpha+mathrmdbeta\n      =fracfmathrmdalphamathrmdalpha+mathrmdbeta+fracgmathrmdbetamathrmdalpha+mathrmdbeta\n      =fracf1+fracmathrmdbetamathrmdalpha+fracgfracmathrmdalphamathrmdbeta+1\n      =fracf1+left(fracmathrmdalphamathrmdbetaright)^-1+fracgfracmathrmdalphamathrmdbeta+1 \n    endaligned\n\n\n\n\n\n","category":"type"},{"location":"api_measurebase/#Base.:|-Tuple{AbstractMeasure, Any}","page":"MeasureBase","title":"Base.:|","text":"(m::AbstractMeasure) | constraint\n\nReturn a new measure by constraining m to satisfy constraint.\n\nNote that the form of constraint will vary depending on the structure of a given measure. For example, a measure over NamedTuples may allow NamedTuple constraints, while another may require constraint to be a predicate or a function returning a real number (in which case the constraint could be considered as the zero-set of that function). \n\nAt the time of this writing, invariants required of this function are not yet settled. Specifically, there's the question of normalization. It's common for conditional distributions to be normalized, but this can often not be expressed in closed form, and can be very expensive to compute. For more general measures, the notion of normalization may not even make sense.\n\nBecause of this, this interface is not yet stable, and users should expect upcoming changes.\n\n\n\n\n\n","category":"method"},{"location":"api_measurebase/#DensityInterface.logdensityof-Tuple{AbstractMeasure, Any}","page":"MeasureBase","title":"DensityInterface.logdensityof","text":"logdensityof(m::AbstractMeasure, x)\n\nCompute the log-density of the measure m at x. Density is always relative, but DensityInterface.jl does not account for this. For compatibility with this, logdensityof for a measure is always implicitly relative to rootmeasure(x). \n\nlogdensityof works by first computing insupport(m, x). If this is true, then unsafe_logdensityof is called. If insupport(m, x) is known to be true, it can be a little faster to directly call unsafe_logdensityof(m, x). \n\nTo compute log-density relative to basemeasure(m) or define a log-density (relative to basemeasure(m) or another measure given explicitly), see logdensity_def. \n\nTo compute a log-density relative to a specific base-measure, see logdensity_rel. \n\n\n\n\n\n","category":"method"},{"location":"api_measurebase/#MeasureBase.:↣-Tuple{Any, Any}","page":"MeasureBase","title":"MeasureBase.:↣","text":"If \n\nμ is an AbstractMeasure or satisfies the Measure interface, and\nk is a function taking values from the support of μ and returning a measure\n\nThen μ ↣ k is a measure, called a monadic bind. In a probabilistic programming language like Soss.jl, this could be expressed as\n\nNote that bind is usually written >>=, but this symbol is unavailable in Julia.\n\nbind = @model μ,k begin \n    x ~ μ\n    y ~ k(x)\n    return y\nend\n\nSee also bind and Bind\n\n\n\n\n\n","category":"method"},{"location":"api_measurebase/#MeasureBase.:⊗-Tuple{Vararg{AbstractMeasure}}","page":"MeasureBase","title":"MeasureBase.:⊗","text":"⊗(μs::AbstractMeasure...)\n\n⊗ is a binary operator for building product measures. This satisfies the law\n\n    basemeasure(μ ⊗ ν) == basemeasure(μ) ⊗ basemeasure(ν)\n\n\n\n\n\n","category":"method"},{"location":"api_measurebase/#MeasureBase.basekleisli","page":"MeasureBase","title":"MeasureBase.basekleisli","text":"For any k::Kleisli, basekleisli is expected to satisfy\n\nbasekleisli(k)(p) == (basemeasure ∘ k)(p)\n\nThe main purpose of basekleisli is to make it efficient to compute\n\nbasemeasure(d::ProductMeasure) = productmeasure(basekleisli(d.f), d.xs)\n\n\n\n\n\n","category":"function"},{"location":"api_measurebase/#MeasureBase.insupport","page":"MeasureBase","title":"MeasureBase.insupport","text":"inssupport(m, x)\ninsupport(m)\n\ninsupport(m,x) computes whether x is in the support of m.\n\ninsupport(m) returns a function, and satisfies\n\ninsupport(m)(x) == insupport(m, x)\n\n\n\n\n\n","category":"function"},{"location":"api_measurebase/#MeasureBase.kleisli","page":"MeasureBase","title":"MeasureBase.kleisli","text":"kleisli(f, M)\nkleisli((f1, f2, ...), M)\n\nA kleisli κ = kleisli(f, m) returns a wrapper around a function f giving the parameters for a measure of type M, such that κ(x) = M(f(x)...) respective κ(x) = M(f1(x), f2(x), ...)\n\nIf the argument is a named tuple (;a=f1, b=f1), κ(x) is defined as M(;a=f(x),b=g(x)).\n\nReference\n\nhttps://en.wikipedia.org/wiki/Markov_kleisli\n\n\n\n\n\n","category":"function"},{"location":"api_measurebase/#MeasureBase.logdensity_def","page":"MeasureBase","title":"MeasureBase.logdensity_def","text":"logdensity_def is the standard way to define a log-density for a new measure. Note that this definition does not include checking for membership in the support; this is instead checked using insupport. logdensity_def is a low-level function, and should typically not be called directly. See logdensityof for more information and other alternatives.\n\n\n\nlogdensity_def(m, x)\n\nCompute the log-density of the measure m at the point x, relative to basemeasure(m), and assuming insupport(m, x).\n\n\n\nlogdensity_def(m1, m2, x)\n\nCompute the log-density of m1 relative to m2 at the point x, assuming insupport(m1, x) and insupport(m2, x).\n\n\n\n\n\n","category":"function"},{"location":"api_measurebase/#MeasureBase.logdensity_rel-Union{Tuple{X}, Tuple{N}, Tuple{M}, Tuple{M, N, X}} where {M, N, X}","page":"MeasureBase","title":"MeasureBase.logdensity_rel","text":"logdensity_rel(m1, m2, x)\n\nCompute the log-density of m1 relative to m2 at x. This function checks whether x is in the support of m1 or m2 (or both, or neither). If x is known to be in the support of both, it can be more efficient to call unsafe_logdensity_rel. \n\n\n\n\n\n","category":"method"},{"location":"api_measurebase/#MeasureBase.paramnames","page":"MeasureBase","title":"MeasureBase.paramnames","text":"paramnames(μ) returns the names of the parameters of μ. This is equivalent to \n\nparamnames(μ) == (keys ∘ params)(μ)\n\nbut depends only on the type. In particular, the default implementation is\n\nparamnames(μ::M) where {M} = paramnames(M)\n\nNew ParameterizedMeasures will automatically have a paramnames method. For other measures, this method is optional, but can be added by defining\n\nparamnames(::Type{M}) where {M} = ...\n\nSee also params\n\n\n\n\n\n","category":"function"},{"location":"api_measurebase/#MeasureBase.params","page":"MeasureBase","title":"MeasureBase.params","text":"params(μ) returns the parameters of a measure μ, as a NamedTuple. The default method is\n\nparams(μ) = NamedTuple()\n\nSee also paramnames\n\n\n\n\n\n","category":"function"},{"location":"api_measurebase/#MeasureBase.proxy","page":"MeasureBase","title":"MeasureBase.proxy","text":"function proxy end\n\nIt's often useful to delegate methods like logdensity and basemeasure to those of a different measure. For example, a Normal{(:μ,:σ)} is equivalent to an affine transformation of a Normal{()}.\n\nWe could just have calls like Normal(μ=2,σ=4) directly construct a transformed measure, but this would make dispatch awkward.\n\n\n\n\n\n","category":"function"},{"location":"api_measurebase/#MeasureBase.rebase-Tuple{Any, Any}","page":"MeasureBase","title":"MeasureBase.rebase","text":"rebase(μ, ν)\n\nExpress μ in terms of a density over ν. Satisfies\n\nbasemeasure(rebase(μ, ν)) == ν\ndensity(rebase(μ, ν)) == 𝒹(μ,ν)\n\n\n\n\n\n","category":"method"},{"location":"api_measurebase/#MeasureBase.rootmeasure-Tuple{Any}","page":"MeasureBase","title":"MeasureBase.rootmeasure","text":"rootmeasure(μ::AbstractMeasure)\n\nIt's sometimes important to be able to find the fix point of a measure under basemeasure. That is, to start with some measure and apply basemeasure repeatedly until there's no change. That's what this does.\n\n\n\n\n\n","category":"method"},{"location":"api_measurebase/#MeasureBase.schema","page":"MeasureBase","title":"MeasureBase.schema","text":"schema(::Type)\n\nschema turns a type into a value that's easier to work with. Example:     julia> nt = (a=(b=[1,2],c=(d=[3,4],e=[5,6])),f=[7,8]);     julia> NT = typeof(nt)     NamedTuple{(:a, :f),Tuple{NamedTuple{(:b, :c),Tuple{Array{Int64,1},NamedTuple{(:d, :e),Tuple{Array{Int64,1},Array{Int64,1}}}}},Array{Int64,1}}}     julia> schema(NT)     (a = (b = Array{Int64,1}, c = (d = Array{Int64,1}, e = Array{Int64,1})), f = Array{Int64,1})\n\n\n\n\n\n","category":"function"},{"location":"api_measurebase/#MeasureBase.unsafe_logdensity_rel-Union{Tuple{X}, Tuple{N}, Tuple{M}, Tuple{M, N, X}} where {M, N, X}","page":"MeasureBase","title":"MeasureBase.unsafe_logdensity_rel","text":"unsafe_logdensity_rel(m1, m2, x)\n\nCompute the log-density of m1 relative to m2 at x, assuming x is known to be in the support of both m1 and m2.\n\nSee also logdensity_rel.\n\n\n\n\n\n","category":"method"},{"location":"api_measurebase/#MeasureBase.unsafe_logdensityof-Union{Tuple{M}, Tuple{M, Any}} where M","page":"MeasureBase","title":"MeasureBase.unsafe_logdensityof","text":"unsafe_logdensityof(m, x)\n\nCompute the log-density of the measure m at x relative to rootmeasure(m). This is \"unsafe\" because it does not check insupport(m, x).\n\nSee also logdensityof.\n\n\n\n\n\n","category":"method"},{"location":"api_measurebase/#MeasureBase.∫-Tuple{Function, AbstractMeasure}","page":"MeasureBase","title":"MeasureBase.∫","text":"∫(f, base::AbstractMeasure)\n\nDefine a new measure in terms of a density f over some measure base.\n\n\n\n\n\n","category":"method"},{"location":"api_measurebase/#MeasureBase.∫exp-Tuple{Function, Any}","page":"MeasureBase","title":"MeasureBase.∫exp","text":"∫exp(f, base::AbstractMeasure)\n\nDefine a new measure in terms of a log-density f over some measure base.\n\n\n\n\n\n","category":"method"},{"location":"api_measurebase/#MeasureBase.𝒹-Tuple{AbstractMeasure, AbstractMeasure}","page":"MeasureBase","title":"MeasureBase.𝒹","text":"𝒹(μ::AbstractMeasure, base::AbstractMeasure)\n\nCompute the Radom-Nikodym derivative of μ with respect to base.\n\n\n\n\n\n","category":"method"},{"location":"api_measuretheory/#MeasureTheory-API","page":"MeasureTheory","title":"MeasureTheory API","text":"","category":"section"},{"location":"api_measuretheory/","page":"MeasureTheory","title":"MeasureTheory","text":"Modules = [MeasureTheory]","category":"page"},{"location":"api_measuretheory/#MeasureTheory.CorrCholesky","page":"MeasureTheory","title":"MeasureTheory.CorrCholesky","text":"CorrCholesky(n)\n\nCholesky factor of a correlation matrix of size n. Transforms n(n-1)2 real numbers to an nn lower-triangular matrix L, such that L*L' is a correlation matrix (positive definite, with unit diagonal).\n\nNotes\n\nIf\n\nz is a vector of n IID standard normal variates,\nσ is an n-element vector of standard deviations,\nC is obtained from CorrCholesky(n),\n\nthen Diagonal(σ) * C.L * z is a zero-centered multivariate normal variate with the standard deviations σ and correlation matrix C.L * C.U.\n\n\n\n\n\n","category":"type"},{"location":"api_measuretheory/#MeasureTheory.For-Union{Tuple{T}, Tuple{Any, Vararg{Any}}} where T","page":"MeasureTheory","title":"MeasureTheory.For","text":"For(f, base...)\n\nFor provides a convenient way to construct a ProductMeasure. There are several options for the base. With Julia's do notation, this can look very similar to a standard for loop, while maintaining semantics structure that's easier to work with.\n\n\n\nFor(f, base::Int...)\n\nWhen one or several Int values are passed for base, the result is treated as depending on CartesianIndices(base). \n\njulia> For(3) do λ Exponential(λ) end |> marginals\n3-element mappedarray(MeasureBase.var\"#17#18\"{var\"#15#16\"}(var\"#15#16\"()), ::CartesianIndices{1, Tuple{Base.OneTo{Int64}}}) with eltype Exponential{(:λ,), Tuple{Int64}}:\n Exponential(λ = 1,)\n Exponential(λ = 2,)\n Exponential(λ = 3,)\n\njulia> For(4,3) do μ,σ Normal(μ,σ) end |> marginals\n4×3 mappedarray(MeasureBase.var\"#17#18\"{var\"#11#12\"}(var\"#11#12\"()), ::CartesianIndices{2, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}}) with eltype Normal{(:μ, :σ), Tuple{Int64, Int64}}:\n Normal(μ = 1, σ = 1)  Normal(μ = 1, σ = 2)  Normal(μ = 1, σ = 3)\n Normal(μ = 2, σ = 1)  Normal(μ = 2, σ = 2)  Normal(μ = 2, σ = 3)\n Normal(μ = 3, σ = 1)  Normal(μ = 3, σ = 2)  Normal(μ = 3, σ = 3)\n Normal(μ = 4, σ = 1)  Normal(μ = 4, σ = 2)  Normal(μ = 4, σ = 3)\n\n\n\nFor(f, base::AbstractArray...)`\n\nIn this case, base behaves as if the arrays are zipped together before applying the map.\n\njulia> For(randn(3)) do x Exponential(x) end |> marginals\n3-element mappedarray(x->Main.Exponential(x), ::Vector{Float64}) with eltype Exponential{(:λ,), Tuple{Float64}}:\n Exponential(λ = -0.268256,)\n Exponential(λ = 1.53044,)\n Exponential(λ = -1.08839,)\n\njulia> For(1:3, 1:3) do μ,σ Normal(μ,σ) end |> marginals\n3-element mappedarray((:μ, :σ)->Main.Normal(μ, σ), ::UnitRange{Int64}, ::UnitRange{Int64}) with eltype Normal{(:μ, :σ), Tuple{Int64, Int64}}:\n Normal(μ = 1, σ = 1)\n Normal(μ = 2, σ = 2)\n Normal(μ = 3, σ = 3)\n\n\n\nFor(f, base::Base.Generator)\n\nFor Generators, the function maps over the values of the generator:\n\njulia> For(eachrow(rand(4,2))) do x Normal(x[1], x[2]) end |> marginals |> collect\n4-element Vector{Normal{(:μ, :σ), Tuple{Float64, Float64}}}:\n Normal(μ = 0.255024, σ = 0.570142)\n Normal(μ = 0.970706, σ = 0.0776745)\n Normal(μ = 0.731491, σ = 0.505837)\n Normal(μ = 0.563112, σ = 0.98307)\n\n\n\n\n\n","category":"method"},{"location":"api_measuretheory/#MeasureTheory.LKJCholesky","page":"MeasureTheory","title":"MeasureTheory.LKJCholesky","text":"LKJCholesky(k=3, η=1.0)\nLKJCholesky(k=3, logη=0.0)\n\nLKJCholesky(k, ...) gives the k×k LKJ distribution (Lewandowski et al 2009) expressed as a Cholesky decomposition. As a special case, for C = rand(LKJCholesky(k=K, η=1.0)) (or equivalently C=rand(LKJCholesky{k}(k=K, logη=0.0))), C.L * C.U is uniform over the set of all K×K correlation matrices. Note, however, that in this case C.L and C.U are not sampled uniformly (because the multiplication is nonlinear).\n\nThe logdensity method for this measure applies for LowerTriangular, UpperTriangular, or Diagonal matrices, and will \"do the right thing\". The logdensity does not check if L*U yields a valid correlation matrix.\n\nValid values are η  0. When η  1, the distribution is unimodal with a peak at I, while 0  η  1 yields a trough. η = 2 is recommended as a vague prior.\n\nAdapted from https://github.com/tpapp/AltDistributions.jl\n\n\n\n\n\n","category":"type"},{"location":"api_measuretheory/#MeasureTheory.asparams","page":"MeasureTheory","title":"MeasureTheory.asparams","text":"asparams build on TransformVariables.as to construct bijections to the parameter space of a given parameterized measure. Because this is only possible for continuous parameter spaces, we allow constraints to assign values to any subset of the parameters.\n\n\n\nasparams(::Type{<:ParameterizedMeasure}, ::StaticSymbol)\n\nReturn a transformation for a particular parameter of a given parameterized measure. For example,\n\njulia> asparams(Normal, static(:σ))\nasℝ₊\n\n\n\nasparams(::Type{<: ParameterizedMeasure{N}}, constraints::NamedTuple) where {N}\n\nReturn a transformation for a given parameterized measure subject to the named tuple constraints. For example,\n\njulia> asparams(Binomial{(:p,)}, (n=10,))\nTransformVariables.TransformTuple{NamedTuple{(:p,), Tuple{TransformVariables.ScaledShiftedLogistic{Float64}}}}((p = as𝕀,), 1)\n\n\n\naspararams(::ParameterizedMeasure)\n\nReturn a transformation with no constraints. For example,\n\njulia> asparams(Normal{(:μ,:σ)})\nTransformVariables.TransformTuple{NamedTuple{(:μ, :σ), Tuple{TransformVariables.Identity, TransformVariables.ShiftedExp{true, Float64}}}}((μ = asℝ, σ = asℝ₊), 2)\n\n\n\n\n\n","category":"function"},{"location":"api_measuretheory/#MeasureTheory.@half-Tuple{Any}","page":"MeasureTheory","title":"MeasureTheory.@half","text":"@half dist([paramnames])\n\nStarting from a symmetric univariate measure dist ≪ Lebesgue(ℝ), create a new measure Halfdist ≪ Lebesgue(ℝ₊). For example,     @half Normal() creates HalfNormal(), and      @half StudentT(ν) creates HalfStudentT(ν).\n\n\n\n\n\n","category":"macro"},{"location":"api_measuretheory/#MeasureTheory.@parameterized-Tuple{Any}","page":"MeasureTheory","title":"MeasureTheory.@parameterized","text":"@parameterized <declaration>\n\nThe <declaration> gives a measure and its default parameters, and specifies its relation to its base measure. For example,     @parameterized Normal(μ,σ) declares the Normal is a measure with default parameters μ and σ. The result is equivalent to\n\nstruct Normal{N,T} <: ParameterizedMeasure{N}\n    par :: NamedTuple{N,T}\nend\nKeywordCalls.@kwstruct Normal(μ,σ)\nNormal(μ,σ) = Normal((μ=μ, σ=σ))\n\nSee KeywordCalls.jl for details on @kwstruct.\n\n\n\n\n\n","category":"macro"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = MeasureTheory","category":"page"},{"location":"#Home","page":"Home","title":"Home","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MeasureTheory.jl is a package for building and reasoning about measures.","category":"page"},{"location":"#Why?","page":"Home","title":"Why?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A distribution (as provided by Distributions.jl) is also called a probability measure, and carries with it the constraint of adding (or integrating) to one. Statistical work usually requires this \"at the end of the day\", but enforcing it at each step of a computation can have considerable overhead. For instance, Bayesian modeling often requires working with unnormalized posterior densities or improper priors.","category":"page"},{"location":"","page":"Home","title":"Home","text":"As a generalization of the concept of volume, measures also have applications outside of probability theory.","category":"page"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install MeasureTheory.jl, open the Julia Pkg REPL (by typing ] in the standard REPL) and run","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add MeasureTheory","category":"page"},{"location":"","page":"Home","title":"Home","text":"To get an idea of the possibilities offered by this package, go to the documentation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"To know more about the underlying theory and its applications to probabilistic programming, check out our JuliaCon 2021 submission.","category":"page"}]
}
