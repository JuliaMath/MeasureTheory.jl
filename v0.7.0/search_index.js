var documenterSearchIndex = {"docs":
[{"location":"intro/","page":"Introduction","title":"Introduction","text":"There are lots of packages for working with probability distributions. But very often, we need to work with \"distributions\" that really aren't. ","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"For example, the correspondence between regularization and Bayesian prior distributions leads naturally to the idea of extending probabilistic programming systems to cover both. But it's easy to come up with a loss function for which the integral of the corresponding \"prior\" is infinite! The result is not really a distirbution. It is, however, still a measure.","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"Even restricted to Bayesian methods, users might sometimes want to use an improper prior. By definition, these cannot be integrated over their domain. But an improper prior is still a measure.","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"In Markov chain Monte Carlo (MCMC), we often work with distributions for which we can only caluculate  the log-density up to an additive constant. Considering this instead as a measure can be helpful. Even better, consdering intermediate computations along the way as computations on measures saves us from computing normalization terms where the end result will discard this anyway.","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"To be clear, that's not to say that we always discard normalizations. Rather, they're considered as belonging to the measure itself, rather than being included in each sub-computation. If measures you work with happen to also be probability distributions, you'll always be able to recover those results.","category":"page"},{"location":"adding/#Adding-a-New-Measure","page":"Adding a New Measure","title":"Adding a New Measure","text":"","category":"section"},{"location":"adding/#Parameterized-Measures","page":"Adding a New Measure","title":"Parameterized Measures","text":"","category":"section"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"This is by far the most common kind of measure, and is especially useful as a way to describe families of probability distributions.","category":"page"},{"location":"adding/#Declaring-a-Parameterized-Measure","page":"Adding a New Measure","title":"Declaring a Parameterized Measure","text":"","category":"section"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"To start, declare a @parameterized. For example, Normal is declared as","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"@parameterized Normal(Œº,œÉ) ‚â™ (1/sqrt2œÄ) * Lebesgue(‚Ñù)","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"[‚Ñù is typed as \\bbR <TAB>]","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"A ParameterizedMeasure can have multiple parameterizations, which as dispatched according to the names of the parameters. The (Œº,œÉ) here specifies names to assign if none are given. So for example,","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"julia> Normal(-3.0, 2.1)\nNormal(Œº = -3.0, œÉ = 2.1)","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"The right side, (1/sqrt2œÄ) * Lebesgue(‚Ñù), gives the base measure. Lebesgue in this case is the technical name for the measure associating an interval of real numbers to its length. The (1/sqrt2œÄ) comes from the normalization constant in the probability density function,","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"f_textNormal(xŒºœÉ) = frac1œÉ sqrt2 pi e^-frac12left(fracx-musigmaright)^2  ","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"Making this part of the base measure allows us to avoid including it in every computation.","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"The ‚â™ (typed as \\ll <TAB>) can be read \"is dominated by\". This just means that any set for which the base measure is zero must also have zero measure in what we're defining.","category":"page"},{"location":"adding/#Defining-a-Log-Density","page":"Adding a New Measure","title":"Defining a Log Density","text":"","category":"section"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"Most computations involve log-densities rather than densities themselves, so these are our first priority. density(d,x) will default to exp(logdensity(d,x)), but you can add a separate method if it's more efficient.","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"The definition is simple:","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"logdensity(d::Normal{()} , x) = - x^2 / 2 ","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"There are a few things here worth noting.","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"First, we dispatch by the names of d (here there are none), and the type of x is not specified.","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"Also, there's nothing here about Œº and œÉ. These location-scale parameters behave exactly the same across lots of distributions, so we have a macro to add them:","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"@ŒºœÉ_methods Normal()","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"Let's look at another example, the Beta distribution. Here the base measure is Lebesgue(ùïÄ) (support is the unit interval). The log-density is","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"function logdensity(d::Beta{(:Œ±, :Œ≤)}, x)\n    return (d.Œ± - 1) * log(x) + (d.Œ≤ - 1) * log(1 - x) - logbeta(d.Œ±, d.Œ≤)\nend","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"Note that when possible, we avoid extra control flow for checking that x is in the support. In applications, log-densities are often evaluated only on the support by design. Such checks should be implemented at a higher level whenever there is any doubt.","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"Finally, note that in both of these examples, the log-density has a relatively direct algebraic form. It's imnportant to have this whenever possible to allow for symbolic manipulation (using libraries like SymolicUtils.jl) and efficient automatic differentiation.","category":"page"},{"location":"adding/#Random-Sampling","page":"Adding a New Measure","title":"Random Sampling","text":"","category":"section"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"For univariate distributions, you should define a Base.rand method that uses three arguments:","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"A Random.AbstractRNG to use for randomization,\nA type to be returned, and\nThe measure to sample from.","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"For our Normal example, this is","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"Base.rand(rng::Random.AbstractRNG, T::Type, d::Normal{()}) = randn(rng, T)","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"Again, for location-scale families, other methods are derived automatically. ","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"For multivariate distributions (or anything that requires heap allocation), you should instead define a Random.rand! method. This also takes three arguments, different from rand:","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"The Random.AbstractRNG,\nThe measure to sample from, and\nWhere to store the result.","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"For example, here's the implementation for ProductMeasure:","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"@propagate_inbounds function Random.rand!(rng::AbstractRNG, d::ProductMeasure, x::AbstractArray)\n    @boundscheck size(d.data) == size(x) || throw(BoundsError)\n\n    @inbounds for j in eachindex(x)\n        x[j] = rand(rng, eltype(x), d.data[j])\n    end\n    x\nend","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"Note that in this example, d.data[j] might itself require allocation. This implementation is likely to change in the future to make it easier to define nested structures without any need for ongoing allocation.","category":"page"},{"location":"adding/#Primitive-Measures","page":"Adding a New Measure","title":"Primitive Measures","text":"","category":"section"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"Most measures are defined in terms of a logdensity with respect to some other measure, its basemeasure. But how is that measure defined? It can't be \"densities all the way down\"; at some point, the chain has to stop.","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"A primitive measure is just a measure that has itself as its own base measure. Note that this also means its log-density is always zero.","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"Here's the implementation of Lebesgue:","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"struct Lebesgue{X} <: AbstractMeasure end\n\nLebesgue(X) = Lebesgue{X}()\n\nbasemeasure(Œº::Lebesgue) = Œº\n\nisprimitive(::Lebesgue) = true\n\nsampletype(::Lebesgue{‚Ñù}) = Float64\nsampletype(::Lebesgue{‚Ñù‚Çä}) = Float64\nsampletype(::Lebesgue{ùïÄ}) = Float64\n\nlogdensity(::Lebesgue, x) = zero(float(x))","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"We haven't yet talked about sampletype. When you call rand without specifying a type, there needs to be a default. That default is the sampletype. This only needs to be defined for primitive measures, because others will fall back on ","category":"page"},{"location":"adding/","page":"Adding a New Measure","title":"Adding a New Measure","text":"sampletype(Œº::AbstractMeasure) = sampletype(basemeasure(Œº))","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = MeasureTheory","category":"page"},{"location":"#MeasureTheory","page":"Home","title":"MeasureTheory","text":"","category":"section"},{"location":"#API","page":"Home","title":"API","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [MeasureTheory]","category":"page"},{"location":"#MeasureTheory.Density","page":"Home","title":"MeasureTheory.Density","text":"struct Density{M,B}\n    Œº::M\n    base::B\nend\n\nFor measures Œº and ŒΩ with Œº‚â™ŒΩ, the density of Œº with respect to ŒΩ (also called the Radon-Nikodym derivative dŒº/dŒΩ) is a function f defined on the support of ŒΩ with the property that for any measurable a ‚äÇ supp(ŒΩ), Œº(a) = ‚à´‚Çê f dŒΩ.\n\nBecause this function is often difficult to express in closed form, there are many different ways of computing it. We therefore provide a formal representation to allow comptuational flexibilty.\n\n\n\n\n\n","category":"type"},{"location":"#MeasureTheory.DensityMeasure","page":"Home","title":"MeasureTheory.DensityMeasure","text":"struct DensityMeasure{F,B} <: AbstractMeasure\n    density :: F\n    base    :: B\nend\n\nA DensityMeasure is a measure defined by a density with respect to some other \"base\" measure \n\n\n\n\n\n","category":"type"},{"location":"#MeasureTheory.LKJL","page":"Home","title":"MeasureTheory.LKJL","text":"The LKJ distribution (Lewandowski et al 2009) for the Cholesky factor L of correlation matrices.\n\nA correlation matrix Œ©=LL has the density Œ©^Œ∑-1. However, it is usually not necessary to construct Œ©, so this distribution is formulated for the Cholesky decomposition L*L', and takes L directly.\n\nNote that the methods does not check if L yields a valid correlation matrix. Valid values are Œ∑  0. When Œ∑  1, the distribution is unimodal at Œ©=I, while 0  Œ∑  1 has a trough. Œ∑ = 2 is recommended as a vague prior. When Œ∑ = 1, the density is uniform in Œ©, but not in L, because of the Jacobian correction of the transformation.\n\nAdapted from https://github.com/tpapp/AltDistributions.jl\n\n\n\n\n\n","category":"type"},{"location":"#MeasureTheory.SuperpositionMeasure","page":"Home","title":"MeasureTheory.SuperpositionMeasure","text":"struct SuperpositionMeasure{X,NT} <: AbstractMeasure\n    components :: NT\nend\n\nSuperposition of measures is analogous to mixture distributions, but (because measures need not be normalized) requires no scaling.\n\nThe superposition of two measures Œº and ŒΩ can be more concisely written as Œº + ŒΩ.\n\n\n\n\n\n","category":"type"},{"location":"#MeasureTheory.WeightedMeasure","page":"Home","title":"MeasureTheory.WeightedMeasure","text":"struct WeightedMeasure{R,M} <: AbstractMeasure\n    logweight :: R\n    base :: M\nend\n\n\n\n\n\n","category":"type"},{"location":"#MeasureTheory.:‚âÉ-Tuple{Any, Any}","page":"Home","title":"MeasureTheory.:‚âÉ","text":"‚âÉ(Œº,ŒΩ)\n\nEquivalence of Measure\n\nMeasures Œº and ŒΩ on the same space X are equivalent, written Œº ‚âÉ ŒΩ, if Œº ‚â™ ŒΩ and ŒΩ ‚â™ Œº. Note that this is often written ~ in the literature, but this is overloaded in probabilistic programming, so we use this alternate notation. \n\nAlso note that equivalence is very different from equality. For two equivalent measures, the sets of non-zero measure will be identical, but what that measure is in each case can be very different. \n\n\n\n\n\n","category":"method"},{"location":"#MeasureTheory.:‚â™","page":"Home","title":"MeasureTheory.:‚â™","text":"‚â™(Œº,ŒΩ)\n\nAbsolute continuity\n\nA measure Œº is absolutely continuous with respect to ŒΩ, written Œº ‚â™ ŒΩ, if ŒΩ(A)==0 implies Œº(A)==0 for every ŒΩ-measurable set A.\n\nLess formally, suppose we have a set A with ŒΩ(A)==0. If Œº(A)‚â†0, then there can be no way to \"reweight\" ŒΩ to get to Œº. We can't make something from nothing.\n\nThis \"reweighting\" is really a density function. If Œº‚â™ŒΩ, then there is some function f that makes Œº == ‚à´(f,ŒΩ) (see the help section for ‚à´).\n\nWe can get this f directly via the Radon-Nikodym derivative, f == ùíπ(Œº,ŒΩ) (see the help section for ùíπ).\n\nNote that ‚â™ is not a partial order, because it is not antisymmetric. That is to say, it's possible (in fact, common) to have two different measures Œº and ŒΩ with Œº ‚â™ ŒΩ and ŒΩ ‚â™ Œº. A simple example of this is \n\nŒº = Normal()\nŒΩ = Lebesgue(‚Ñù)\n\nWhen ‚â™ holds in both directions, the measures Œº and ŒΩ are equivalent, written Œº ‚âÉ ŒΩ. See the help section for ‚âÉ for more information.\n\n\n\n\n\n","category":"function"},{"location":"#MeasureTheory.For-Tuple{Any, Vararg{Any, N} where N}","page":"Home","title":"MeasureTheory.For","text":"For(f, base...)\n\nFor provides a convenient way to construct a ProductMeasure. There are several options for the base. With Julia's do notation, this can look very similar to a standard for loop, while maintaining semantics structure that's easier to work with.\n\n\n\nFor(f, base::Int...)\n\nWhen one or several Int values are passed for base, the result is treated as depending on CartesianIndices(base). \n\njulia> For(3) do Œª Exponential(Œª) end |> marginals\n3-element mappedarray(MeasureTheory.var\"#17#18\"{var\"#15#16\"}(var\"#15#16\"()), ::CartesianIndices{1, Tuple{Base.OneTo{Int64}}}) with eltype Exponential{(:Œª,), Tuple{Int64}}:\n Exponential(Œª = 1,)\n Exponential(Œª = 2,)\n Exponential(Œª = 3,)\n\njulia> For(4,3) do Œº,œÉ Normal(Œº,œÉ) end |> marginals\n4√ó3 mappedarray(MeasureTheory.var\"#17#18\"{var\"#11#12\"}(var\"#11#12\"()), ::CartesianIndices{2, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}}) with eltype Normal{(:Œº, :œÉ), Tuple{Int64, Int64}}:\n Normal(Œº = 1, œÉ = 1)  Normal(Œº = 1, œÉ = 2)  Normal(Œº = 1, œÉ = 3)\n Normal(Œº = 2, œÉ = 1)  Normal(Œº = 2, œÉ = 2)  Normal(Œº = 2, œÉ = 3)\n Normal(Œº = 3, œÉ = 1)  Normal(Œº = 3, œÉ = 2)  Normal(Œº = 3, œÉ = 3)\n Normal(Œº = 4, œÉ = 1)  Normal(Œº = 4, œÉ = 2)  Normal(Œº = 4, œÉ = 3)\n\n\n\nFor(f, base::AbstractArray...)`\n\nIn this case, base behaves as if the arrays are zipped together before applying the map.\n\njulia> For(randn(3)) do x Exponential(x) end |> marginals\n3-element mappedarray(x->Main.Exponential(x), ::Vector{Float64}) with eltype Exponential{(:Œª,), Tuple{Float64}}:\n Exponential(Œª = -0.268256,)\n Exponential(Œª = 1.53044,)\n Exponential(Œª = -1.08839,)\n\njulia> For(1:3, 1:3) do Œº,œÉ Normal(Œº,œÉ) end |> marginals\n3-element mappedarray((:Œº, :œÉ)->Main.Normal(Œº, œÉ), ::UnitRange{Int64}, ::UnitRange{Int64}) with eltype Normal{(:Œº, :œÉ), Tuple{Int64, Int64}}:\n Normal(Œº = 1, œÉ = 1)\n Normal(Œº = 2, œÉ = 2)\n Normal(Œº = 3, œÉ = 3)\n\n\n\nFor(f, base::Base.Generator)\n\nFor Generators, the function maps over the values of the generator:\n\njulia> For(eachrow(rand(4,2))) do x Normal(x[1], x[2]) end |> marginals |> collect\n4-element Vector{Normal{(:Œº, :œÉ), Tuple{Float64, Float64}}}:\n Normal(Œº = 0.255024, œÉ = 0.570142)\n Normal(Œº = 0.970706, œÉ = 0.0776745)\n Normal(Œº = 0.731491, œÉ = 0.505837)\n Normal(Œº = 0.563112, œÉ = 0.98307)\n\n\n\n\n\n","category":"method"},{"location":"#MeasureTheory.asparams","page":"Home","title":"MeasureTheory.asparams","text":"asparams build on TransformVariables.as to construct bijections to the parameter space of a given parameterized measure. Because this is only possible for continuous parameter spaces, we allow constraints to assign values to any subset of the parameters.\n\n\n\nasparams(::Type{<:ParameterizedMeasure}, ::Val{::Symbol})\n\nReturn a transformation for a particular parameter of a given parameterized measure. For example,\n\njulia> asparams(Normal, Val(:œÉ))\nas‚Ñù‚Çä\n\n\n\nasparams(::Type{<: ParameterizedMeasure{N}}, constraints::NamedTuple) where {N}\n\nReturn a transformation for a given parameterized measure subject to the named tuple constraints. For example,\n\njulia> asparams(Binomial{(:p,)}, (n=10,))\nTransformVariables.TransformTuple{NamedTuple{(:p,), Tuple{TransformVariables.ScaledShiftedLogistic{Float64}}}}((p = asùïÄ,), 1)\n\n\n\naspararams(::ParameterizedMeasure)\n\nReturn a transformation with no constraints. For example,\n\njulia> asparams(Normal{(:Œº,:œÉ)})\nTransformVariables.TransformTuple{NamedTuple{(:Œº, :œÉ), Tuple{TransformVariables.Identity, TransformVariables.ShiftedExp{true, Float64}}}}((Œº = as‚Ñù, œÉ = as‚Ñù‚Çä), 2)\n\n\n\n\n\n","category":"function"},{"location":"#MeasureTheory.basemeasure","page":"Home","title":"MeasureTheory.basemeasure","text":"basemeasure(Œº)\n\nMany measures are defined in terms of a logdensity relative to some base measure. This makes it important to be able to find that base measure.\n\nFor measures not defined in this way, we'll typically have basemeasure(Œº) == Œº.\n\n\n\n\n\n","category":"function"},{"location":"#MeasureTheory.isprimitive","page":"Home","title":"MeasureTheory.isprimitive","text":"isprimitive(::AbstractMeasure)\nisprimitive(::Type{M}) where {M<:AbstractMeasure}\n\nMost measures are defined in terms of other measures, for example using a density or a pushforward. Those that are not are considered (in this library, it's not a general measure theory thing) to be primitive. The canonical example of a primitive measure is Lebesgue(X) for some X.\n\n\n\n\n\n","category":"function"},{"location":"#MeasureTheory.kernel","page":"Home","title":"MeasureTheory.kernel","text":"kernel(f, M)\nkernel((f1, f2, ...), M)\n\nA kernel Œ∫ = kernel(f, m) returns a wrapper around a function f giving the parameters for a measure of type M, such that Œ∫(x) = M(f(x)...) respective Œ∫(x) = M(f1(x), f2(x), ...)\n\nIf the argument is a named tuple (;a=f1, b=f1), Œ∫(x) is defined as M(;a=f(x),b=g(x)).\n\nReference\n\nhttps://en.wikipedia.org/wiki/Markov_kernel\n\n\n\n\n\n","category":"function"},{"location":"#MeasureTheory.logdensity","page":"Home","title":"MeasureTheory.logdensity","text":"logdensity(Œº::AbstractMeasure{X}, x::X)\n\nCompute the logdensity of the measure Œº at the point x. This is the standard way to define logdensity for a new measure. the base measure is implicit here, and is understood to be basemeasure(Œº).\n\nMethods for computing density relative to other measures will be\n\n\n\n\n\n","category":"function"},{"location":"#MeasureTheory.representative-Tuple{Any}","page":"Home","title":"MeasureTheory.representative","text":"representative(Œº::AbstractMeasure) -> AbstractMeasure\n\nWe need to be able to compute Œº ‚â™ ŒΩ for each Œº and ŒΩ. To do this directly would require a huge number of methods (quadratic in the number of defined measures). \n\nThis function is a way around that. When defining a new measure Œº, you should also find some equivalent measure œÅ that's \"as primitive as possible\". \n\nIf possible, œÅ should be a PrimitiveMeasure, or a Product of these. If not, it should be a  transform (Pushforward or Pullback) of a PrimitiveMeasure (or Product of these). \n\n\n\n\n\n","category":"method"},{"location":"#MeasureTheory.‚à´-Tuple{Any, AbstractMeasure}","page":"Home","title":"MeasureTheory.‚à´","text":"‚à´(f, base::AbstractMeasure; log=true)\n\nDefine a new measure in terms of a density f over some measure base. If log=true (the default), f is considered as a log-density.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureTheory.ùíπ-Tuple{AbstractMeasure, AbstractMeasure}","page":"Home","title":"MeasureTheory.ùíπ","text":"ùíπ(Œº::AbstractMeasure, base::AbstractMeasure; log=true)\n\nCompute the Radom-Nikodym derivative (or its log, if log=true) of Œº with respect to base.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureTheory.@domain-Tuple{Any, Any}","page":"Home","title":"MeasureTheory.@domain","text":"@domain(name, T)\n\nDefines a new singleton struct T, and a value name for building values of that type.\n\nFor example, @domain ‚Ñù RealNumbers is equivalent to\n\nstruct RealNumbers <: AbstractDomain end\n\nexport ‚Ñù\n\n‚Ñù = RealNumbers()\n\nBase.show(io::IO, ::RealNumbers) = print(io, \"‚Ñù\")\n\n\n\n\n\n","category":"macro"},{"location":"#MeasureTheory.@half-Tuple{Any}","page":"Home","title":"MeasureTheory.@half","text":"@half dist([paramnames])\n\nStarting from a symmetric univariate measure dist ‚â™ Lebesgue(‚Ñù), create a new measure Halfdist ‚â™ Lebesgue(‚Ñù‚Çä). For example,\n\n@half Normal()\n\ncreates HalfNormal(), and \n\n@half StudentT(ŒΩ)\n\ncreates HalfStudentT(ŒΩ).\n\n\n\n\n\n","category":"macro"},{"location":"#MeasureTheory.@parameterized-Tuple{Any}","page":"Home","title":"MeasureTheory.@parameterized","text":"@parameterized <declaration>\n\nThe <declaration> gives a measure and its default parameters, and specifies its relation to its base measure. For example,\n\n@parameterized Normal(Œº,œÉ)\n\ndeclares the Normal is a measure with default parameters Œº and œÉ. The result is equivalent to\n\nstruct Normal{N,T} <: ParameterizedMeasure{N}\n    par :: NamedTuple{N,T}\nend\n\nKeywordCalls.@kwstruct Normal(Œº,œÉ)\n\nNormal(Œº,œÉ) = Normal((Œº=Œº, œÉ=œÉ))\n\nSee KeywordCalls.jl for details on @kwstruct.\n\n\n\n\n\n","category":"macro"},{"location":"#MeasureTheory.@primitive-Tuple{Any}","page":"Home","title":"MeasureTheory.@primitive","text":"@primitive T\n\nDeclare that every measure of type T should be considered \"primitive\". A measure is primitive is it is not defined in terms of another measure. Common examples are Lebesgue and Counting measures.\n\nNote that this is not a general measure-theoretic term, but is specific to the MeasureTheory.jl implementation.\n\n\n\n\n\n","category":"macro"}]
}
